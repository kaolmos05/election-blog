---
title: Blog Post 8
author: Kelly Olmos
date: '2024-10-28'
slug: blog-post-8
categories: []
tags: []
---

In this week's blog post I will continue refining my election forecast model. 

### Shocks 

In class this week, we learned about how shocks affect election outcomes. Shocks are defined as 
unforeseeable events such as natural disasters and pandemics. 

Achen and Bartels (2017) argue that voters hold incumbent politicians accountable for natural disasters and other uncontrollable events. They call this, "blind retrospection", to describe when voter punish the incumbent for their conditions even if the incumbent is not responsible for the unsatisfactory conditions voters feel. They draw upon the historical example of the 1916 shark attacks the decreased Woodrow Wilson's vote share in counties where the shark attacks occurred. 

However, Fowler and Hall (2018) challenge Achen and Bartels (2017). They point out that Achen and Bartels have conveniently left some counties out of their model. When those counties are included in Fowler and Hall's replication, they reach a different conclusion than Achen and Bartels, fining no evidence that events like shark attacks significantly influence electoral outcomes.

If Achen and Bartels' conclusion were accurate, it would indicate that voters do not vote rationally because their decision is influenced by unrelated and random events that the incumbent cannot control. Healy and Malhotra (2010) provide evidence for the contrary. In their observational study of tornado incidences and its impact on electoral outcomes, they conclude that voters reward and punish their government in accordance with how their government handles the disaster rather than the occurrence of the tornado. 

```{r }
#' @title GOV 1347: Week 7 (Ground Game) Laboratory Session
#' @author Matthew E. Dardet
#' @date October 15, 2024

####----------------------------------------------------------#
#### Preamble
####----------------------------------------------------------#

# Load libraries.
## install via `install.packages("name")`
library(geofacet)
library(usmap)
library(ggpubr)
library(ggthemes)
library(haven)
library(stringr)
library(kableExtra)
library(maps)
library(mgcv)
library(mgcViz)
library(RColorBrewer)
library(scales)
library(sf)
library(spData)
library(stargazer)
library(tidygeocoder)
library(tidyverse)
library(tigris)
library(tmap)
library(tmaptools)
library(viridis)
library(broom)
library(knitr)

## set working directory here
# setwd("~")

```

```{r }

####----------------------------------------------------------#
#### Read, merge, and process data.
####----------------------------------------------------------#

# Read popular vote datasets. 
d_popvote <- read_csv("popvote_1948_2020.csv")
d_state_popvote <- read_csv("state_popvote_1948_2020.csv")
d_state_popvote[d_state_popvote$state == "District of Columbia",]$state <- "District Of Columbia"

# Read elector distribution dataset. 
d_ec <- read_csv("corrected_ec_1948_2024.csv")

# Read polling data. 
d_polls <- read_csv("national_polls_1968-2024.csv")
d_state_polls <- read_csv("state_polls_1968-2024.csv")

# Read turnout data. 
d_turnout <- read_csv("state_turnout_1980_2022.csv")

# Read county turnout. 
d_county_turnout <- read_csv("county_turnout.csv")

# Read state-level demographics.
d_state_demog <- read_csv("demographics.csv")

d_state_gdp <- read_csv("cleaned_state_gdp.csv")

hurricanes <- read_csv("hurricanes_1996_2016.csv")
d_state_unemployment <- read_csv("State_unemployment.csv")
```
```{r}
#Process state unemployment for my model's needs
#Making all columns except State numeric variables
d_state_unemployment[ , -1] <- lapply(d_state_unemployment[ , -1], as.numeric)

d_state_unemployment <- d_state_unemployment |>
  pivot_longer(
    cols = -State,
    names_to = "date",
    values_to = "unemployment" #as a rate
  ) |>
  separate(date, into = c("month", "year"), sep = " ") |>
  rename(state = State) |>
  filter(month == "Sep")
  
d_state_unemployment$year <- as.numeric(d_state_unemployment$year)


d_state_unemployment
```


```{r data processing for gdp}
#Process gdp growth for model's need 

d_state_gdp <- d_state_gdp |>
  select(year, state, q2_gdp_growth) |>
  drop_na()

d_state_gdp
```

```{r personal dis income}

personal_dis_income <- read_csv("personalIncomeState.csv", skip = 3)

regions_to_remove <- c(
  "United States ", "Far West ", "New England", "Mideast", "Great Lakes",
  "Plains", "Southeast", "Southwest", "Rocky Mountain", "District of Columbia"
)

#cleaning the data and processing it
personal_dis_income <- personal_dis_income |>
  rename(state = GeoName) |>
  mutate(across(-state, ~ as.numeric(gsub("\\(NA\\)", "", .)))) |>
  pivot_longer(
    cols = -c(state, GeoFips),  
    names_to = "year", 
    values_to = "disposable_income" 
  ) |>
  mutate(year = as.numeric(year)) |>
  mutate(state = gsub("\\*", "", state)) |>
  filter(year >= 2008) |>
  filter(!is.na(state) & !(state %in% regions_to_remove))

personal_dis_income

```
```{r hurricanes}
hurricanes <- hurricanes |>
  select(YEAR, MONTH_NAME, STATE, DAMAGE_PROPERTY) |>
  drop_na() |> 
  mutate(
    year = YEAR,
    month = MONTH_NAME,
    state = str_to_title(tolower(STATE)),
    damage = DAMAGE_PROPERTY,
    damage = case_when(
      grepl("K", damage) ~ as.numeric(sub("K", "", damage)) * 1000,
      grepl("M", damage) ~ as.numeric(sub("M", "", damage)) * 1000000,
      grepl("B", damage) ~ as.numeric(sub("B", "", damage)) * 1000000000
    )
  ) |>
  select(year, month, damage, state)


  
hurricanes
```
### Effect of Hurricanes on Incumbent Vote Share

Hurricanes are an example of a shock. I ran a regression model to estimate the relationship between the economic damages a hurricane causes and how it impacts the incumbent's vote share. 

The coefficient for damage (in dollars) is 2.62e-10, meaning that for every additional dollar of hurricane damage, the incumbent's vote share is expected to increase by a very small amount (0.000000000262 percentage points). Regardless, the p-value is nearly, 0.50, which indicates that the relationship between hurricane damage and the incumbent’s vote share change is not statistically significant. This means that voters do not punish a candidate more if the hurricane brings about more damage which suggests that there are other variables that impact the incumbent's vote share. 

In graph 1 below, it is evident there this is barely a relationship between the two variables. In fact, there is a slight positive relationship. One possible explanation is that when hurricanes cause a lot of damage, federal agencies and other states often step in to provide aid. This response may create a perception that the local government is effectively responding to the hurricane's damage, which may result in a slight increase in support for the incumbent. 

There are limitations to this data analysis, I excluded all hurricanes where there was no economic damage figure provided. On another note, I tried to use this data for my election forecast model but it would not work because I'm training my data on 2024 and this data set only goes up to 2016. Since shocks are unexpected, I can't reliably predict the type of shock voters may experience in 2024 and its effects. Whereas, for other variables in my model I can use lagged values, I can't for this type of data due to its unexpected and random nature. 
\
```{r}

hurricane_data <- hurricanes |>
  left_join(d_state_popvote, by = c("state", "year")) |>
  group_by(state, year) |>
  mutate(
    incumbent_party = case_when(
      D_pv2p_lag1 > R_pv2p_lag1 ~ "Democrat",
      R_pv2p_lag1 > D_pv2p_lag1 ~ "Republican"
    ),
    incumbent_vote_share = case_when(
      incumbent_party == "Democrat" ~ D_pv2p,
      incumbent_party == "Republican" ~ R_pv2p,
    ),
    incumbent_vote_share_change = incumbent_vote_share - case_when(
      incumbent_party == "Democrat" ~ D_pv2p_lag1,
      incumbent_party == "Repubbllican" ~ R_pv2p_lag1
    )
    ) |>
  drop_na()

hurricane_data

hurricane_incumbent_model <- lm(incumbent_vote_share_change ~ damage, data = hurricane_data)

ggplot(hurricane_data, aes(x = damage, y = incumbent_vote_share_change)) +
  geom_point(alpha = 0.3) + 
  geom_smooth(method = "lm", color = "dodgerblue", se = TRUE) + 
  labs(
    title = "Effect of Hurricane Economic Damage on Incumbent Vote Share Change",
    x = "Hurricane Damage (Dollars)",
    y = "Change in Incumbent Vote Share",
    caption = "Graph 1"
  ) 
```

### Update to my Election Forecast Prediction 

In this week's model, I decided to swap out state GDP for state Q2 GDP growth because GDP growth seems like a better measure of a state's economy. Some states, like California, have a much larger GDP due to its population and commerce but it intuitively makes sense that comparing state GDP that varies can be standardized by comparing GDP growth (%). I removed the mean poll averages because I think it was causing multicollinearity and potentially over weighing the poll data. For the same reason, I opted to test unemployment and realized that it was causing economic indicators to have disproportionate impact on the election outcome. The variables I kept in my regression model are latest_pollav_DEM, D_pv2p_lag1, q2_gdp_growth, and disposable_income. Below is the predicted outcome. I'm surprised that by changing state GDP to state Q2 GDP growth that it has made all swing states go for Harris when in the previous blog post, six out of the seven states went for Trump. I did remove some variables this week but it shouldn't have caused such a swing in the electoral college outcome because those variables removed were conveying similar information to the other variables in the regression model. Regardless, the table below shows the predicted two party vote share and every state is within margin of error and could go for either candidate which reflects how close this election is. 

```{r}

d_polls <- read_csv("national_polls_1968-2024.csv")
d_state_polls <- read_csv("state_polls_1968-2024.csv")

# Process state-level polling data.
d_pollav_state <- d_state_polls |>
  group_by(year, state, party) |>
  mutate(mean_pollav = mean(poll_support, na.rm = TRUE)) |>
  top_n(1, poll_date) |>
  rename(latest_pollav = poll_support) |>
  select(-c(weeks_left, days_left, poll_date, candidate, before_convention)) |>
  pivot_wider(names_from = party, values_from = c(latest_pollav, mean_pollav))

# Merge data.
d <- d_pollav_state |>
  left_join(d_state_popvote, by = c("year", "state")) |>
  left_join(d_popvote |> filter(party == "democrat"), by = "year") |>
  left_join(d_turnout, by = c("year", "state")) |>
  left_join(personal_dis_income, by = c("state", "year")) |>
  left_join(d_state_gdp, by = c("state", "year")) |>
  left_join(d_state_unemployment, by = c("state", "year")) |>
  filter(year >= 1980) |>
  ungroup()


# Sequester states for which we have polling data for 2024.
states.2024 <- unique(d$state[d$year == 2024])
states.2024 <- states.2024[-which(states.2024 == "Nebraska Cd 2")]
d <- d |>
  filter(state %in% states.2024)

d
# Separate into training and testing for simple poll prediction model. 
d.train <- d |> filter(year <= 2024) |> select(year, state, D_pv2p, latest_pollav_DEM, mean_pollav_DEM, D_pv2p_lag1, D_pv2p_lag2, q2_gdp_growth, disposable_income) |> drop_na()

names(d.train)
d.test <- d |> filter(year == 2024) |> select(year, state, D_pv2p, latest_pollav_DEM, mean_pollav_DEM, D_pv2p_lag1, D_pv2p_lag2, q2_gdp_growth, disposable_income)

d.test

# Add back in lagged vote share for 2024. 
t <- d |> 
  filter(year >= 2016) |> 
  arrange(year) |> 
  group_by(state) |> 
  mutate(
    D_pv2p_lag1 = lag(D_pv2p, 1),
    R_pv2p_lag1 = lag(R_pv2p, 1), 
    D_pv2p_lag2 = lag(D_pv2p, 2),
    R_pv2p_lag2 = lag(R_pv2p, 2),
    disposable_income = lag(disposable_income, 1)) |> 
  filter(year == 2024) |> 
  select(state, year, D_pv2p, D_pv2p_lag1, R_pv2p_lag1, D_pv2p_lag2, R_pv2p_lag2, disposable_income) 

t

d.test <- d.test|>
  left_join(t, by = c("state", "year"), suffix = c("", ".t")) |>
  mutate(
    D_pv2p_lag1 = ifelse(is.na(D_pv2p_lag1), D_pv2p_lag1.t, D_pv2p_lag1),
    D_pv2p_lag2 = ifelse(is.na(D_pv2p_lag2), D_pv2p_lag2.t, D_pv2p_lag2),
    disposable_income = ifelse(is.na(disposable_income), disposable_income.t, disposable_income),
    D_pv2p = ifelse(is.na(D_pv2p), D_pv2p.t, D_pv2p), 
  ) |>
  select(-ends_with(".t"))

d.test
# Standard frequentist linear regression. 
reg.ols <- lm(D_pv2p ~ latest_pollav_DEM + D_pv2p_lag1 + q2_gdp_growth + disposable_income, data = d.train)

summary(reg.ols)
pred.ols.dem <- predict(reg.ols, newdata = d.test)
pred.ols.dem

# Create dataset to summarize winners and EC vote distributions. 
win_pred <- data.frame(state = d.test$state,
                       year = rep(2024, length(d.test$state)),
                       simp_pred_dem = pred.ols.dem,
                       simp_pred_rep = 100 - pred.ols.dem) |> 
            mutate(winner = ifelse(simp_pred_dem > simp_pred_rep, "Democrat", "Republican")) |>
            left_join(d_ec, by = c("state", "year"))

win_pred

win_pred |> 
  filter(winner == "Democrat") |> 
  select(state)

win_pred |> 
  filter(winner == "Republican") |> 
  select(state)

win_pred |> 
  group_by(winner) |> 
  summarize(n = n(), ec = sum(electors))

# Create data set to summarize winners and EC vote distributions. 
win_pred <- data.frame(state = d.test$state,
                       year = rep(2024, length(d.test$state)),
                       simp_pred_dem = pred.ols.dem,
                       simp_pred_rep = 100 - pred.ols.dem) |> 
            mutate(winner = ifelse(simp_pred_dem > simp_pred_rep, "Democrat", "Republican")) |> left_join(d_ec, by = c("state", "year"))


#### monte carlo simulations so i can create confidence intervals 
# Set the number of simulations
m <- 1e4  # 10,000 simulations

residual_se <- summary(reg.ols)$sigma

pred_dem <- predict(reg.ols, newdata = d.test)

n_states <- nrow(d.test)

pred.mat <- data.frame(
  state = rep(d.test$state, times = m),
  year = rep(2024, times = m * n_states),
  simp_pred_dem = numeric(m * n_states),
  simp_pred_rep = numeric(m * n_states)
)

j <- 1
for (i in 1:m) {
  if (i %% 1000 == 0) {
    print(paste("Simulation", i))
  }
    simulated_errors <- rnorm(n_states, mean = 0, sd = residual_se)
    pred_dem_sim <- pred_dem + simulated_errors
    pred_dem_sim <- pmin(pmax(pred_dem_sim, 0), 100)
    pred_rep_sim <- 100 - pred_dem_sim
  
  idx <- j:(i * n_states)
  pred.mat$simp_pred_dem[idx] <- pred_dem_sim
  pred.mat$simp_pred_rep[idx] <- pred_rep_sim
  
  j <- j + n_states
}

pred.mat <- pred.mat %>%
  mutate(
    winner = ifelse(simp_pred_dem > simp_pred_rep, "Democrat", "Republican")
  )
####


electoral_outcomes <- pred.mat |>
  group_by(state) |>
  summarize(
    mean_dem = mean(simp_pred_dem),
    mean_rep = mean(simp_pred_rep),
    sd_dem = sd(simp_pred_dem),
    sd_rep = sd(simp_pred_rep),
    lower_dem = mean_dem - 1.96 * sd_dem,
    upper_dem = mean_dem + 1.96 * sd_dem,
    lower_rep = mean_rep - 1.96 * sd_rep,
    upper_rep = mean_rep + 1.96 * sd_rep
  ) |>
  mutate(
    winner = ifelse(mean_dem > mean_rep, "Democrat", "Republican")
  )

view(electoral_outcomes)


#Since this only gives us the results for 19 states, it includes all swing states so a simple calculation with lagged vote will allow me to fill in the winner for the rest of the US
#list of all states 
all_states <- state.name

missing_states <- setdiff(all_states, unique(electoral_outcomes$state))
missing_states

#created a new data frame for the missing states 
missing_data <- d_state_popvote |>
  filter(state %in% missing_states) |>
  filter(year == 2020) |>
  select(state, D_pv2p_lag1, R_pv2p_lag1)

#Create the winner column based on lagged vote shares. Since these are not swing states, using the outcome from the last election reflects whethere it is a blue/red state
missing_data <- missing_data|>
  mutate(
    winner = ifelse(D_pv2p_lag1 > R_pv2p_lag1, "Democrat", "Republican")
  )

ec_2024 <- d_ec |>
  filter(year == 2024) |> 
  select(state, electors)
  


#Combine datasets 
all_states_pred <- bind_rows(electoral_outcomes, missing_data) |>
  left_join(ec_2024, by = "state")

winner <- all_states_pred |>
  group_by(winner) |>
  summarize(total_electors = sum(electors))

winner

#DC not included so add 3 for Dem electoral college count
election_results <- tibble(
  party = c("Democrat", "Republican"),
  total_electors = c(winner$total_electors[1] + 3, winner$total_electors[2])
)

kable(election_results)


plot_usmap(data = all_states_pred, regions = "states", values = "winner") + scale_fill_manual(
    values = c("Democrat" = "blue", "Republican" = "red"),
    name = "Predicted Winner"
  ) +
  labs(title = "Electoral College Predictions") 

```

```{r}
#how close is this race?
electoral_outcomes_usa <- electoral_outcomes |>
  group_by(state) |>
  summarise(sum(mean_dem))

electoral_outcomes_usa

electoral_outcomes_close <- electoral_outcomes |>
  filter(state %in% c("Arizona", "Georgia", "Michigan", "Nevada", "North Carolina", "Pennsylvania", "Wisconsin")) |>
  select(state, mean_dem, mean_rep, lower_dem, upper_dem,lower_rep, upper_rep, winner)

kable(electoral_outcomes_close)
```


### References 

Achen, Christopher, and Larry Bartels. “Blind retrospection: Electoral responses to droughts, floods, and shark attacks.” Democracy for Realists, 31 Dec. 2017, pp. 116–145, https://doi.org/10.1515/9781400888740-007. 

Fowler, Anthony, and Andrew B. Hall. “Do shark attacks influence presidential elections? reassessing a prominent finding on voter competence.” The Journal of Politics, vol. 80, no. 4, Oct. 2018, pp. 1423–1437, https://doi.org/10.1086/699244. 

Healy, Andrew, and Neil Malhotra. “Random events, economic losses, and retrospective voting: Implications for democratic competence.” Quarterly Journal of Political Science, vol. 5, no. 2, 22 Aug. 2010, pp. 193–208, https://doi.org/10.1561/100.00009057. 