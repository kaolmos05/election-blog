---
title: 'Post-Election Reflection '
author: Kelly Olmos
date: '2024-11-18'
slug: post-election-reflection
categories: []
tags: []
---

```{r set up}
library(censable)
library(geofacet)
library(ggpubr)
library(ggthemes)
library(haven)
library(kableExtra)
library(maps)
library(mgcv)
library(mgcViz)
library(RColorBrewer)
library(readstata13)
library(scales)
library(sf)
library(spData)
library(stargazer)
library(tidygeocoder)
library(tidyverse)
library(tigris)
library(tmap)
library(tmaptools)
library(viridis)
library(geofacet)
library(glmnet)
library(usmap)
library(ggpubr)
library(ggthemes)
library(haven)
library(kableExtra)
library(maps)
library(mgcv)
library(mgcViz)
library(RColorBrewer)
library(scales)
library(sf)
library(spData)
library(stargazer)
library(tidygeocoder)
library(tidyverse)
library(tigris)
library(tmap)
library(tmaptools)
library(viridis)
library(caret)
library(broom)
library(knitr)
library(dplyr)
library(Metrics)
```

### My Model and its Predictions

I ran a state-level ordinary least squares regression. For the non-battleground states, I used the lagged vote shares to predict which candidate would win that particular state. I used polling data, fundamentals (Q2 gdp growth, and demographics (age, race, and education level) to predict which candidate would win in each state. Since there was not enough polling data for all states, I focused on the ones that didâ€“battleground states. I ran an elastic net on my OLS regression model to deal with multicollinearity. Some variables like race and education level may be correlated. Additionally I ran 10,000 simulations to create 95% confidence intervals. 

The total list of predictors in my OLS regression is: latest_pollav_DEM, D_pv2p_lag1,  D_pv2p_lag2 +, q2_gdp_growth, white, black, american_indian, asian_pacific_islander, less_than_college, college, hispanic, age_18_to_29, age_30_to_64, age_65_75plus. 

My model predicted a Trump victory with 285 electoral college votes and 258 electoral college votes for Harris. My model inaccurately predicted that Georgia and Arizona would go for Harris. 


```{r}
state_pres_vote <- read_csv("state_votes_pres_2024.csv")

state_pres_vote <- state_pres_vote |>
  select(state = "Geographic Name", Trump = "Donald J. Trump", Harris = "Kamala D. Harris") |>
  mutate(
    Trump = as.numeric(Trump),
    Harris = as.numeric(Harris),
    D2pv = (Harris/(Trump + Harris))*100,
    Winner = as.factor(ifelse(D2pv > 50, "Harris", "Trump"))
    ) |> 
  drop_na()

my_predictions <- read_csv("my_predictions.csv")

my_predictions <- my_predictions|>
  mutate(
    pred_Winner = as.factor(ifelse(my_prediction > 50, "Harris", "Trump"))
  ) |>
  left_join(state_pres_vote, by = c("state")) |>
  select(state, D2pv, my_prediction, Winner, pred_Winner) |> 
  filter(state %in% c("Arizona", "North Carolina", "Nevada", "Pennsylvania", "Michigan", "Wisconsin", "Georgia"))


kable(my_predictions)
  
```

In the bar chart below, I have plotted my predicted Democratic two-party vote share compared to actual Democratic two-party vote share in the battleground states. 

```{r bar chart}
bar_chart <- my_predictions |>
  select(state, D2pv, my_prediction) |>
  pivot_longer(cols = c(D2pv, my_prediction), 
               names_to = "type", 
               values_to = "D2pv_value")

ggplot(bar_chart, aes(x = state, y = D2pv_value, fill = type)) +
  geom_bar(stat = "identity", position = position_dodge(width = 0.8)) +
  scale_fill_manual(values = c("D2pv" = "darkgreen", "my_prediction" = "red"), 
                    labels = c("D2pv (%)", "Predicted D2pv (%)")) +
  labs(
    title = "Predicted vs Actual Democratic Two-Party Vote Share",
    x = "State",
    y = "D2pv",
    fill = "Key"
  ) 
```

### Accuracy of my Model 

To quantify the accuracy of my model, I calculated the MSE, RMSE, MAE, and Bias. The findings are presented in the table below. 

The average squared difference between predicted and actual Democratic vote shares is 2.104.

The average error is approximately 1.45 percentage points.

The average absolute error between my predicted and actual Democratic vote share is 1.13 percentage points

My bias score indicates that I overestimated the Democratic party vote share by 0.76 points on average. 

```{r metrics}

Mse <- mse(my_predictions$D2pv, my_predictions$my_prediction)
Mse

Rmse <- rmse(my_predictions$D2pv, my_predictions$my_prediction)
Rmse 

Mae <- mae(my_predictions$D2pv, my_predictions$my_prediction)
Mae

Bias <- bias(my_predictions$D2pv, my_predictions$my_prediction)
Bias

accuracy <- data.frame(
  metric = c("MSE", "RMSE", "MAE", "Bias"),
  value = c(Mse, Rmse, Mae, Bias)
)

kable(accuracy)
```

I performed a state-level analysis to test how well my model worked in certain states. My model performed the worst in Arizona where I overestimated the Democratic party vote share by a little over three percentage points. The model performed the best in Pennsylvania where the MSE value was 0.003. 

```{r state-level analysis}

state_level_analysis <- my_predictions |>
  group_by(state) |> 
  mutate(
    mse = mse(D2pv, my_prediction),
    rmse = rmse(D2pv, my_prediction),
    mae = mae(D2pv, my_prediction),
    bias = bias(D2pv, my_prediction)
  )

kable(state_level_analysis)
```

Below is a heat map with the error metrics visualized. The bias in my model did not disproportionately overestimate the Democratic two-party vote share. In four states: Arizona, Georgia, Nevada, and North Carolina it did overestimate the Democratic two party vote share. However, my model did overestimate Trump's winning margin in Michigan, Pennsylvania, and Wisconsin although the bias for Trump was less on average. I found this interesting because the states were I overestimated Trump's winning margin were also the ones that were hypothesized by political pundits to go for Harris. Michigan and Wisconsin provided hope for Harris because they used to make up the blue wall. I knew immediately that my model was predicting Arizona wrong because polling data showed that out of the seven battleground states, Arizona was one of the least likely states to go for Harris.

To investigate why there is no direction in bias for my model I will go back to my OLS regression results and explore which of my variables might have affected my model. 
```{r}
state_level_analysis_long <- state_level_analysis |>
  pivot_longer(cols = c(mse, rmse, mae, bias), names_to = "Metric", values_to = "Value")

#https://r-charts.com/correlation/heat-map-ggplot2/
   
ggplot(state_level_analysis_long, aes(state, Metric, fill = Value)) +
  geom_tile() +
  geom_text(aes(label = round(Value, 3)), color = "black") +
  scale_fill_gradient(low = "darkseagreen1", high = "darkseagreen4") +
  labs(
    title = "State-Level Analysis of Metrics",
    x = "State",
    y = "Metric",
    fill = "Value"
  ) 
```

I created a confusion matrix. The confusion matrix shows that I did not predict Harris would win any state that Trump actually won. It is easier to understand the errors because Trump had a clean sweep of the battleground states and my model only predicted he would win 5 which means that the error 
comes from predicting a Harris win in Arizona and Georgia where Trump won. 

```{r}

table("Actual" = my_predictions$Winner, 
      "Prediction" = my_predictions$pred_Winner)

confusion_matrix <- confusionMatrix(data= my_predictions$Winner, reference = my_predictions$pred_Winner)
```
### Reflection

I used a combination of demographics, polling data, and economic indicators to predict the electoral college results and the democratic two-party vote share in the battleground states with a OLS regression. One of the things that immediately stands out to me is that I may have made my model too complex. Although my model did not do a terrible job predicting the outcome of the election, I recognize from a statistical perspective that there were too many variables and some of them are not independent of one another which led to multicollinearity. For example, I have race and education level variables which are likely correlated in my linear regression. Additionally, I had a high r-squared value, around the high nineties which suggests that my model may be overfitting. To test whether the OLS regression model is not effective for predicting electoral outcomes, I could create an ensemble model or I could test the performance of my model with less variables. 

In terms of the components of my model, I knew including demographic variables was a risk because the Kim & Zilinsky article we read a couple of weeks ago claims that five key demographic attributes can predict a voter's choice about 68% of the time. I only use three of those key attributes so it may be even lower for my model in terms of its predicting capacity. Post-election exit polls show that Trump made gains with most demographic groups which indicates that demographic attributes may not have great predicting capacity in this election. To test this out I removed race variables from my model and this is how the update model performed below. 

```{r no race vars}
updates <- read_csv("updates.csv")

update_one <- my_predictions |>
  left_join(updates,  by = c("state"))

update_one
Mse1 <- mse(update_one$D2pv, update_one$'update 1')
Mse1

Rmse1 <- rmse(update_one$D2pv, update_one$'update 1')
Rmse1 

Mae1 <- mae(update_one$D2pv, update_one$'update 1')
Mae1

Bias1 <- bias(update_one$D2pv, update_one$'update 1')
Bias1

accuracy1 <- data.frame(
  metric = c("MSE", "RMSE", "MAE", "Bias"),
  value = c(Mse1, Rmse1, Mae1, Bias1)
)

kable(accuracy1)

state_level_analysis1 <- update_one |>
  group_by(state) |> 
  mutate(
    mse = mse(D2pv, `update 1`),
    rmse = rmse(D2pv, `update 1`),
    mae = mae(D2pv, `update 1`),
    bias = bias(D2pv, `update 1`)
  )

kable(state_level_analysis1)

state_level_analysis_long1 <- state_level_analysis1 |>
  pivot_longer(cols = c(mse, rmse, mae, bias), names_to = "Metric", values_to = "Value")

ggplot(state_level_analysis_long1, aes(state, Metric, fill = Value)) +
  geom_tile() +
  geom_text(aes(label = round(Value, 3)), color = "black") +
  scale_fill_gradient(low = "darkseagreen1", high = "darkseagreen4") +
  labs(
    title = "State-Level Analysis of Metrics",
    x = "State",
    y = "Metric",
    fill = "Value"
  ) 
```

After removing the race variables from my model, it does perform better, accurately predicting a Trump victory in all battleground states, but has started to underestimate the Democratic two-party vote share in six out of the seven battleground states which I thought was interesting. The model once again overestimates the Harris margin which makes me wonder what about the variables and Arizona is so unique that it is not being captured by the variables in my model. I predict it has to do with the quality of polls in Arizona potentially overestimating Harris support in Arizona. The graphic below shows that Arizona substantially shifted to the right. To test whether the polls overestimated Harris support in the states like Arizona, we will have to wait until the votes are all counted and final election results are available and compare them to what the polls predicted. 

Ultimately, it is not possible to know the impact of race until the voter files are available because exit polls are known to be less reliable. Once the voter file is available, I can test how demographic groups voted in the 2024 election. 

```{r}
# Read 2024 results datasets. 
d_state_2024 <- read_csv("state_votes_pres_2024.csv")[-1, 1:6]
d_county_2024 <- read_csv("county_votes_pres_2024.csv")[-1, 1:6]
d_county_2020 <- read_csv("county_votes_pres_2020.csv")[-1, 1:6]

# Process 2024 state and county-level data. 
d_state_2024 <- d_state_2024 |> 
  mutate(FIPS = as.numeric(FIPS), 
         votes_trump = as.numeric(`Donald J. Trump`), 
         votes_harris = as.numeric(`Kamala D. Harris`), 
         votes = as.numeric(`Total Vote`), 
         trump_pv = votes_trump/votes, 
         harris_pv = votes_harris/votes, 
         trump_2pv = votes_trump/(votes_trump + votes_harris), 
         harris_2pv = votes_harris/(votes_trump + votes_harris)) |> 
  mutate(winner = case_when(votes_trump > votes_harris ~ "REP", 
                            .default = "DEM")) |> 
  select(FIPS, `Geographic Name`, `Geographic Subtype`, votes_trump, votes_harris, votes, 
         winner, trump_pv, harris_pv, trump_2pv, harris_2pv)

d_county_2024 <- d_county_2024 |>
  mutate(FIPS = as.numeric(FIPS),
         votes_trump = as.numeric(`Donald J. Trump`), 
         votes_harris = as.numeric(`Kamala D. Harris`), 
         votes = as.numeric(`Total Vote`), 
         trump_pv = votes_trump/votes, 
         harris_pv = votes_harris/votes, 
         trump_2pv = votes_trump/(votes_trump + votes_harris), 
         harris_2pv = votes_harris/(votes_trump + votes_harris)) |> 
  mutate(winner = case_when(votes_trump > votes_harris ~ "REP", 
                            .default = "DEM")) |> 
  select(FIPS, `Geographic Name`, `Geographic Subtype`, votes_trump, votes_harris, votes, 
         winner, trump_pv, harris_pv, trump_2pv, harris_2pv)

d_county_2020 <- d_county_2020 |> 
  mutate(FIPS = as.numeric(FIPS),
         votes_trump_2020 = as.numeric(`Donald J. Trump`), 
         votes_biden_2020 = as.numeric(`Joseph R. Biden Jr.`), 
         votes_2020 = as.numeric(`Total Vote`), 
         trump_pv_2020 = votes_trump_2020/votes_2020, 
         biden_pv_2020 = votes_biden_2020/votes_2020, 
         trump_2pv_2020 = votes_trump_2020/(votes_trump_2020 + votes_biden_2020), 
         biden_2pv_2020 = votes_biden_2020/(votes_trump_2020 + votes_biden_2020)) |> 
  mutate(winner_2020 = case_when(votes_trump_2020 > votes_biden_2020 ~ "REP", 
                            .default = "DEM")) |> 
  select(FIPS, `Geographic Name`, `Geographic Subtype`, votes_trump_2020, votes_biden_2020, votes_2020, 
         winner_2020, trump_pv_2020, biden_pv_2020, trump_2pv_2020, biden_2pv_2020)
counties_2024 <- counties(cb = TRUE, resolution = "5m", year = 2023) |> 
  shift_geometry() |> 
  mutate(GEOID = as.numeric(GEOID)) |> 
  left_join(d_county_2024, by = c("GEOID" = "FIPS")) |> 
  left_join(d_county_2020, by = c("GEOID" = "FIPS")) |>
  mutate(shift = (trump_pv - trump_pv_2020) * 100, 
         shift_dir = case_when(shift > 0 ~ "REP", 
                               shift < 0 ~ "DEM", 
                               TRUE ~ "No Change"),
         centroid = st_centroid(geometry), 
         centroid_long = st_coordinates(centroid)[,1],
         centroid_lat = st_coordinates(centroid)[,2],
         scale_factor = 1e4, 
         end_long = centroid_long + scale_factor * shift,
         end_lat = centroid_lat + scale_factor * shift) |>
  drop_na()
county_pop_2024 <- read_csv("PopulationEstimates.csv") |> 
  mutate(FIPStxt = as.numeric(FIPStxt)) |>
  select(FIPStxt, POP_ESTIMATE_2023)
counties_2024 <- counties_2024 |> 
  left_join(county_pop_2024, by = c("GEOID" = "FIPStxt"))
counties_2024 |> 
  filter(STATE_NAME == "Arizona") |> 
  ggplot() +
  geom_sf(fill = "gray95", color = "darkgrey") +  # Base map
  geom_text(aes(x = centroid_long, y = centroid_lat-1.5e4, label = NAME),
            size = 2,  # Adjust size as needed
            color = "black", hjust = 0.5, vjust = -0.5) + 
  geom_curve(aes(x = centroid_long, 
                 y = centroid_lat,
                 xend = end_long, 
                 yend = end_lat,
                 color = shift_dir),
             arrow = arrow(length = unit(0.1, "cm"), type = "closed"),  # Smaller arrowhead
             curvature = 0.2,  # Add a slight curve to each arrow
             size = 0.3) +
  scale_color_manual(values = c("DEM" = "blue", "REP" = "red")) +
  theme_void() +
  labs(title = "Presidential Voting Shifts by County in Arizona",
       subtitle = "Democratic vs. Republican Gains")
```
I also looked back at my original OLS regression model and decided to select only the variables that were statistically significant. Interestingly, it was all of my race variables and the variable containing the latest polling data. I ran the same model again with these variables and this was the outcome (shown below).

```{r}
updates <- read_csv("updates.csv")

update_one <- my_predictions |>
  left_join(updates,  by = c("state"))

update_one
Mse2 <- mse(update_one$D2pv, update_one$'update 2')
Mse2

Rmse2 <- rmse(update_one$D2pv, update_one$'update 2')
Rmse2

Mae2 <- mae(update_one$D2pv, update_one$'update 2')
Mae2

Bias2 <- bias(update_one$D2pv, update_one$'update 2')
Bias2

accuracy2 <- data.frame(
  metric = c("MSE", "RMSE", "MAE", "Bias"),
  value = c(Mse2, Rmse2, Mae2, Bias2)
)

kable(accuracy2)

state_level_analysis2 <- update_one |>
  group_by(state) |> 
  mutate(
    mse = mse(D2pv, `update 2`),
    rmse = rmse(D2pv, `update 2`),
    mae = mae(D2pv, `update 2`),
    bias = bias(D2pv, `update 2`)
  )

kable(state_level_analysis2)

state_level_analysis_long2 <- state_level_analysis2 |>
  pivot_longer(cols = c(mse, rmse, mae, bias), names_to = "Metric", values_to = "Value")

ggplot(state_level_analysis_long2, aes(state, Metric, fill = Value)) +
  geom_tile() +
  geom_text(aes(label = round(Value, 3)), color = "black") +
  scale_fill_gradient(low = "darkseagreen1", high = "darkseagreen4") +
  labs(
    title = "State-Level Analysis of Metrics",
    x = "State",
    y = "Metric",
    fill = "Value"
  ) 
```
When I only include the variables that were statistically significant, the model performs better than my original better and the one updates without race variables. The aggregate level MSE value is 0.17 which is close to 0, indicating very good accuracy. I think this is really interesting because the Kim & Zilinsky article did not show the demographic attributes had high predicting capacity. This latest iteration of my model shows that polling data and race variables predict this election outcome very well. 

What does this say about the Q2 GDP growth? This was my only economic variable in my model. I did consider including unemployment but in the end opted to exclude because it was showing multicollinearity with Q2 GDP growth. When I added Q2 GDP growth to both of the updated models it increased the Democratic two-party vote share to the extent where it flipped states. I think this is the result of Trump's perception as a successful business man. A couple of weeks ago, I listened to The Daily and in this episode, there were many clips of interviews with Nevada voters who said they were voting for Trump because of the economy. Inflation in 2022 and 2023 has tainted the economic image of the Biden administration and Harris was not able to escape that. I can test whether voter's perception of the economy is a better predictor for electoral outcomes by switching out Q2 GDP growth with inflation or consumer economic index and include it in my model and see how it performs. 

In a future iteration of this model I would also like to account for incumbency and make it an interaction variable with some economic indicator. Post-election discussion that I've heard suggests that Trump won because voters were dissatisfied with the economy and wanted change. I think an interaction variable could have captured public dissatisfaction and performed better. 



